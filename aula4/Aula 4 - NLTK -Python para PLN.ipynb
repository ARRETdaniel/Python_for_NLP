{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\danie\\anaconda3\\envs\\pytorch\\lib\\site-packages (3.6.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\danie\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\danie\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\danie\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (2021.9.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\danie\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6269066313af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-316d6efec44a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmac_morpho\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "len(nltk.corpus.mac_morpho.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jersei',\n",
       " 'atinge',\n",
       " 'média',\n",
       " 'de',\n",
       " 'Cr$',\n",
       " '1,4',\n",
       " 'milhão',\n",
       " 'em',\n",
       " 'a',\n",
       " 'venda',\n",
       " 'de',\n",
       " 'a',\n",
       " 'Pinhal',\n",
       " 'em',\n",
       " 'São',\n",
       " 'Paulo']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.mac_morpho.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3c255a536eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmac_morpho\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.corpus.mac_morpho.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ('de', 'PREP'), ('Cr$', 'CUR'), ('1,4', 'NUM'), ('milhão', 'N'), ('em', 'PREP|+'), ('a', 'ART'), ('venda', 'N'), ('de', 'PREP|+'), ('a', 'ART'), ('Pinhal', 'NPROP'), ('em', 'PREP'), ('São', 'NPROP'), ('Paulo', 'NPROP')], [('Programe', 'V'), ('sua', 'PROADJ'), ('viagem', 'N'), ('a', 'PREP|+'), ('a', 'ART'), ('Exposição', 'NPROP'), ('Nacional', 'NPROP'), ('do', 'NPROP'), ('Zebu', 'NPROP'), (',', ','), ('que', 'PRO-KS-REL'), ('começa', 'V'), ('dia', 'N'), ('25', 'N|AP')], ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.mac_morpho.tagged_sents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK -TOKENIZAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Com',\n",
       " 'um',\n",
       " 'passe',\n",
       " 'de',\n",
       " 'Eli',\n",
       " 'Manning',\n",
       " 'para',\n",
       " 'Plaxico',\n",
       " 'Burress',\n",
       " 'a',\n",
       " '39',\n",
       " 'segundos',\n",
       " 'do',\n",
       " 'fim',\n",
       " 'o',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Giants',\n",
       " 'anotou',\n",
       " 'o',\n",
       " 'touchdown',\n",
       " 'decisivo',\n",
       " 'e',\n",
       " 'derrubou',\n",
       " 'o',\n",
       " 'favorito',\n",
       " 'New',\n",
       " 'England',\n",
       " 'Patriots',\n",
       " 'por',\n",
       " '17',\n",
       " 'a',\n",
       " '14',\n",
       " 'neste',\n",
       " 'domingo',\n",
       " 'em',\n",
       " 'Glendale',\n",
       " 'no',\n",
       " 'Super',\n",
       " 'Bowl',\n",
       " 'XLII']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "'''path = 'corpus_teste.txt'\n",
    "infale = open(path, 'r')\n",
    "texto = infale'''\n",
    "texto = (\"Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.\")\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(texto)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joao'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z]\\w*')\n",
    "#tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
    "\n",
    "tokens = tokenizer.tokenize(texto)\n",
    "#tokens\n",
    "tokens.append('joao')\n",
    "tokens1 = tokenizer.tokenize(texto)\n",
    "#tokens1\n",
    "#freq = nltk.FreqDist(tokens)\n",
    "#freq.most_common()\n",
    "#for token in tokens1:\n",
    "#  if tokens != tokens1:\n",
    "#    keys = tokens\n",
    "\n",
    "differences = set(tokens).symmetric_difference(set(tokens1))\n",
    "print(differences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-194362d50cdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(texto)\n",
    "freg = nltk.FreqDist(tokens)\n",
    "freg.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-227b2ab73d25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(texto)\n",
    "freg = nltk.FreqDist(tokens)\n",
    "freg.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corpus_teste.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8527afbd6de1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpus_teste.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corpus_teste.txt'"
     ]
    }
   ],
   "source": [
    "corpus = open('corpus_teste.txt').read()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "freg = nltk.FreqDist(tokens)\n",
    "freg.most_common()\n",
    "#print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giants batem os Patriots no Super Bowl XLII\n",
      "AzarÃµes acabam com a invencibilidade de New England e ficam com o tÃ­tulo da temporada\n",
      "04/02/2008 - 01h07m - Atualizado em 04/02/2008 - 09h49m\n",
      "\n",
      "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII. O resultado, uma das maiores zebras da histÃ³ria do Super Bowl, acabou com a temporada perfeita de Tom Brady e companhia, que esperavam fazer histÃ³ria ao levantar o trofÃ©u da NFL sem sofrer uma derrota no ano. \n",
      "\n",
      "A vitÃ³ria dos Giants, porÃ©m, tambÃ©m ficarÃ¡ para a histÃ³ria. Pela primeira vez, irmÃ£os quarterbacks triunfam no Super Bowl em temporadas consecutivas. No ano passado, Peyton Manning, irmÃ£o de Eli, chegou ao tÃ­tulo mÃ¡ximo da NFL pelo Indianapolis Colts.\n",
      "\n",
      "A partida\n",
      "\n",
      "Os Giants comeÃ§aram com a posse de bola, e mostraram logo que iriam alongar ao mÃ¡ximo suas posses de bola. Misturando corridas com Brandon Jacobs e passes curtos, o time de Nova York chegou Ã  red zone logo na primeira campanha. O avanÃ§o, no entanto, parou na linha de 17 jardas e Lawrence Tynes converteu o field goal de 32 jardas para abrir o placar.\n",
      "\n",
      "Eli Manning e companhia ficaram 9m54s com a bola, mas o ataque dos Patriots nÃ£o entrou em campo frio. Logo no retorno do kickoff, o running back Laurence Maroney avanÃ§ou 43 jardas, deixando Tom Brady em boa posiÃ§Ã£o. Com passes curtos, os Patriots chegaram Ã  linha de 17 jardas e, graÃ§as a uma penalidade (interferÃªncia de passe) do linebacker Antonio Pierce, alcanÃ§aram a linha de uma jarda. Maroney avanÃ§ou pelo chÃ£o e anotou o primeiro touchdown do jogo.\n",
      "\n",
      "Os Giants pareciam rumo Ã  virada na campanha seguinte. Manning achou Amani Toomer para um avanÃ§o de 38 jardas, e o time de Nova York entrou novamente na red zone. Com a bola na linha de 14 jardas dos Patriots, os Giants sofreram um revÃ©s. Manning passou para Steve Smith, que soltou a bola. Ellis Hobbs aproveitou, tomou a posse para os Patriots, e avanÃ§ou 23 jardas. \n",
      "\n",
      "A defesa de Nova York manteve o jogo equilibrado. Com dois sacks seguidos, os Giants forÃ§aram o punt e recuperaram a bola. Mas a campanha seguinte provou ser outra decepÃ§Ã£o para Nova York. O time chegou Ã  linha de 25 jardas, mas Manning sofreu um sack e cometeu um fumble, e o ataque voltou para a linha de 39 jardas, nÃ£o conseguindo pontuar mais uma vez.\n",
      "\n",
      "Os Patriots tiveram uma Ãºltima chance de marcar antes do intervalo, mas, a 22 segundos do fim do segundo perÃ­odo, Brady foi novamente sacado. Desta vez, ele cometeu o fumble e os Giants tomaram a posse de bola. Manning tentou um passe longo, de 50 jardas, nos Ãºltimos segundos, mas nÃ£o teve sucesso. \n",
      "\n",
      "O jogo continuou amarrado no terceiro quarto, com as defesas levando a melhor sobre os ataques. A Ãºnica chance de pontuar do perÃ­odo foi dos Patriots, que chegaram Ã  linha de 31 jardas dos Giants. O tÃ©cnico Bill Bellichick, porÃ©m, optou por uma quarta descida em vez de um field goal. Brady tentou um passe para Jabar Gaffney, mas nÃ£o conseguiu completar.\n",
      "\n",
      "O Ãºltimo perÃ­odo comeÃ§ou arrasador para os Giants. na primeira jogada, Manning achou o tight end Kevin Boss, para um incrÃ­vel avanÃ§o de 45 jardas, que deixou o time na linha de 35 dos Patriots. Outro lanÃ§amento, desta vez para Steve Smith, marcou o avanÃ§o atÃ© a linha de 12 jardas. Duas jogadas depois, David Tyree pegou um passe de cinco jardas na end zone para anotar o touchdown e virar o jogo.\n",
      "\n",
      "Na hora da decisÃ£o, o ataque dos Patriots voltou a funcionar. Com uma sÃ©rie de passes curtos e variados, Brady achou Wes Welker, Randy Moss e Kevin Faulk seguidas vezes atÃ© chegar Ã  red zone. A 2m45s do fim, o quarterback conectou mais uma vez com Moss, que se desmarcou e ficou livre na lateral direita da end zone.\n",
      "\n",
      "Quando os fÃ£s de New England jÃ¡ comemoravam a vitÃ³ria, o inesperado aconteceu. Em uma jogada incrÃ­vel, Eli Manning se soltou de dois marcadores que o seguravam pela camisa e, na corrida, lanÃ§ou para Amani Toomer. O wide receiver, bem marcado, saltou e conseguiu a fazer recepÃ§Ã£o para um avanÃ§o de 32 jardas, deixando os Giants na linha de 24 de New England.\n",
      "\n",
      "Quatro jogadas depois, a 39 segundos do fim, Manning achou Plaxico Burress na end zone para conseguir o touchdown do tÃ­tulo.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-29c3dc61a01c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#corpus = open('corpus_teste.txt').read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[a-zA-Z]\\w*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnova_lista\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "#corpus = open('corpus_teste.txt').read()\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "nova_lista = []\n",
    "for token in tokens:\n",
    "  nova_lista.append(token.lower())\n",
    "#print(nova_lista)\n",
    "freq = nltk.FreqDist(nova_lista)\n",
    "freq.most_common()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'a',\n",
       " 'o',\n",
       " 'que',\n",
       " 'e',\n",
       " 'é',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'não',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'à',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'já',\n",
       " 'eu',\n",
       " 'também',\n",
       " 'só',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'até',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'você',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'às',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'nós',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estávamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivéramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivéssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'há',\n",
       " 'havemos',\n",
       " 'hão',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvéramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvéssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverá',\n",
       " 'houveremos',\n",
       " 'houverão',\n",
       " 'houveria',\n",
       " 'houveríamos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'são',\n",
       " 'era',\n",
       " 'éramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fôramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fôssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'serão',\n",
       " 'seria',\n",
       " 'seríamos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tém',\n",
       " 'tinha',\n",
       " 'tínhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivéramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivéssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terá',\n",
       " 'teremos',\n",
       " 'terão',\n",
       " 'teria',\n",
       " 'teríamos',\n",
       " 'teriam']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk.corpus.stopwords.words('portuguese'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-70ae0a82058d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[a-zA-Z]\\w*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnova_lista\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "nova_lista = []\n",
    "\n",
    "for token in tokens:\n",
    "  if token.lower() not in stopwords:\n",
    "   nova_lista.append(token.lower())\n",
    "\n",
    "#print(nova_lista)\n",
    "freq = nltk.FreqDist(nova_lista)\n",
    "freq.most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jardas', 15),\n",
       " ('giants', 11),\n",
       " ('patriots', 10),\n",
       " ('manning', 10),\n",
       " ('linha', 10),\n",
       " ('avanã', 8),\n",
       " ('bola', 7),\n",
       " ('vez', 6),\n",
       " ('zone', 6),\n",
       " ('new', 5),\n",
       " ('passe', 5),\n",
       " ('york', 5),\n",
       " ('brady', 5),\n",
       " ('super', 4),\n",
       " ('bowl', 4),\n",
       " ('england', 4),\n",
       " ('tã', 4),\n",
       " ('eli', 4),\n",
       " ('segundos', 4),\n",
       " ('fim', 4),\n",
       " ('touchdown', 4),\n",
       " ('time', 4),\n",
       " ('nova', 4),\n",
       " ('nã', 4),\n",
       " ('jogo', 4),\n",
       " ('achou', 4),\n",
       " ('end', 4),\n",
       " ('tulo', 3),\n",
       " ('histã³ria', 3),\n",
       " ('m', 3),\n",
       " ('primeira', 3),\n",
       " ('chegou', 3),\n",
       " ('aram', 3),\n",
       " ('posse', 3),\n",
       " ('logo', 3),\n",
       " ('passes', 3),\n",
       " ('curtos', 3),\n",
       " ('red', 3),\n",
       " ('campanha', 3),\n",
       " ('ataque', 3),\n",
       " ('perã', 3),\n",
       " ('odo', 3),\n",
       " ('xlii', 2),\n",
       " ('temporada', 2),\n",
       " ('plaxico', 2),\n",
       " ('burress', 2),\n",
       " ('anotou', 2),\n",
       " ('tom', 2),\n",
       " ('companhia', 2),\n",
       " ('fazer', 2),\n",
       " ('nfl', 2),\n",
       " ('ano', 2),\n",
       " ('vitã³ria', 2),\n",
       " ('porã', 2),\n",
       " ('irmã', 2),\n",
       " ('mã', 2),\n",
       " ('ximo', 2),\n",
       " ('comeã', 2),\n",
       " ('field', 2),\n",
       " ('goal', 2),\n",
       " ('entrou', 2),\n",
       " ('maroney', 2),\n",
       " ('deixando', 2),\n",
       " ('chegaram', 2),\n",
       " ('seguinte', 2),\n",
       " ('amani', 2),\n",
       " ('toomer', 2),\n",
       " ('novamente', 2),\n",
       " ('s', 2),\n",
       " ('steve', 2),\n",
       " ('smith', 2),\n",
       " ('soltou', 2),\n",
       " ('dois', 2),\n",
       " ('cometeu', 2),\n",
       " ('fumble', 2),\n",
       " ('voltou', 2),\n",
       " ('pontuar', 2),\n",
       " ('chance', 2),\n",
       " ('desta', 2),\n",
       " ('tentou', 2),\n",
       " ('conseguiu', 2),\n",
       " ('jogada', 2),\n",
       " ('kevin', 2),\n",
       " ('incrã', 2),\n",
       " ('vel', 2),\n",
       " ('lanã', 2),\n",
       " ('atã', 2),\n",
       " ('jogadas', 2),\n",
       " ('moss', 2),\n",
       " ('batem', 1),\n",
       " ('azarãµes', 1),\n",
       " ('acabam', 1),\n",
       " ('invencibilidade', 1),\n",
       " ('ficam', 1),\n",
       " ('h07m', 1),\n",
       " ('atualizado', 1),\n",
       " ('h49m', 1),\n",
       " ('decisivo', 1),\n",
       " ('derrubou', 1),\n",
       " ('favorito', 1),\n",
       " ('neste', 1),\n",
       " ('domingo', 1),\n",
       " ('glendale', 1),\n",
       " ('resultado', 1),\n",
       " ('maiores', 1),\n",
       " ('zebras', 1),\n",
       " ('acabou', 1),\n",
       " ('perfeita', 1),\n",
       " ('esperavam', 1),\n",
       " ('levantar', 1),\n",
       " ('trofã', 1),\n",
       " ('u', 1),\n",
       " ('sofrer', 1),\n",
       " ('derrota', 1),\n",
       " ('tambã', 1),\n",
       " ('ficarã', 1),\n",
       " ('quarterbacks', 1),\n",
       " ('triunfam', 1),\n",
       " ('temporadas', 1),\n",
       " ('consecutivas', 1),\n",
       " ('passado', 1),\n",
       " ('peyton', 1),\n",
       " ('indianapolis', 1),\n",
       " ('colts', 1),\n",
       " ('partida', 1),\n",
       " ('mostraram', 1),\n",
       " ('iriam', 1),\n",
       " ('alongar', 1),\n",
       " ('posses', 1),\n",
       " ('misturando', 1),\n",
       " ('corridas', 1),\n",
       " ('brandon', 1),\n",
       " ('jacobs', 1),\n",
       " ('entanto', 1),\n",
       " ('parou', 1),\n",
       " ('lawrence', 1),\n",
       " ('tynes', 1),\n",
       " ('converteu', 1),\n",
       " ('abrir', 1),\n",
       " ('placar', 1),\n",
       " ('ficaram', 1),\n",
       " ('m54s', 1),\n",
       " ('campo', 1),\n",
       " ('frio', 1),\n",
       " ('retorno', 1),\n",
       " ('kickoff', 1),\n",
       " ('running', 1),\n",
       " ('back', 1),\n",
       " ('laurence', 1),\n",
       " ('boa', 1),\n",
       " ('posiã', 1),\n",
       " ('graã', 1),\n",
       " ('penalidade', 1),\n",
       " ('interferãªncia', 1),\n",
       " ('linebacker', 1),\n",
       " ('antonio', 1),\n",
       " ('pierce', 1),\n",
       " ('alcanã', 1),\n",
       " ('jarda', 1),\n",
       " ('chã', 1),\n",
       " ('primeiro', 1),\n",
       " ('pareciam', 1),\n",
       " ('rumo', 1),\n",
       " ('virada', 1),\n",
       " ('sofreram', 1),\n",
       " ('revã', 1),\n",
       " ('passou', 1),\n",
       " ('ellis', 1),\n",
       " ('hobbs', 1),\n",
       " ('aproveitou', 1),\n",
       " ('tomou', 1),\n",
       " ('defesa', 1),\n",
       " ('manteve', 1),\n",
       " ('equilibrado', 1),\n",
       " ('sacks', 1),\n",
       " ('seguidos', 1),\n",
       " ('forã', 1),\n",
       " ('punt', 1),\n",
       " ('recuperaram', 1),\n",
       " ('provou', 1),\n",
       " ('ser', 1),\n",
       " ('outra', 1),\n",
       " ('decepã', 1),\n",
       " ('sofreu', 1),\n",
       " ('sack', 1),\n",
       " ('conseguindo', 1),\n",
       " ('ltima', 1),\n",
       " ('marcar', 1),\n",
       " ('antes', 1),\n",
       " ('intervalo', 1),\n",
       " ('segundo', 1),\n",
       " ('sacado', 1),\n",
       " ('tomaram', 1),\n",
       " ('longo', 1),\n",
       " ('ltimos', 1),\n",
       " ('sucesso', 1),\n",
       " ('continuou', 1),\n",
       " ('amarrado', 1),\n",
       " ('terceiro', 1),\n",
       " ('quarto', 1),\n",
       " ('defesas', 1),\n",
       " ('levando', 1),\n",
       " ('melhor', 1),\n",
       " ('sobre', 1),\n",
       " ('ataques', 1),\n",
       " ('nica', 1),\n",
       " ('cnico', 1),\n",
       " ('bill', 1),\n",
       " ('bellichick', 1),\n",
       " ('optou', 1),\n",
       " ('quarta', 1),\n",
       " ('descida', 1),\n",
       " ('jabar', 1),\n",
       " ('gaffney', 1),\n",
       " ('completar', 1),\n",
       " ('ltimo', 1),\n",
       " ('arrasador', 1),\n",
       " ('tight', 1),\n",
       " ('boss', 1),\n",
       " ('deixou', 1),\n",
       " ('outro', 1),\n",
       " ('amento', 1),\n",
       " ('marcou', 1),\n",
       " ('duas', 1),\n",
       " ('david', 1),\n",
       " ('tyree', 1),\n",
       " ('pegou', 1),\n",
       " ('cinco', 1),\n",
       " ('anotar', 1),\n",
       " ('virar', 1),\n",
       " ('hora', 1),\n",
       " ('decisã', 1),\n",
       " ('funcionar', 1),\n",
       " ('sã', 1),\n",
       " ('rie', 1),\n",
       " ('variados', 1),\n",
       " ('wes', 1),\n",
       " ('welker', 1),\n",
       " ('randy', 1),\n",
       " ('faulk', 1),\n",
       " ('seguidas', 1),\n",
       " ('vezes', 1),\n",
       " ('chegar', 1),\n",
       " ('m45s', 1),\n",
       " ('quarterback', 1),\n",
       " ('conectou', 1),\n",
       " ('desmarcou', 1),\n",
       " ('ficou', 1),\n",
       " ('livre', 1),\n",
       " ('lateral', 1),\n",
       " ('direita', 1),\n",
       " ('fã', 1),\n",
       " ('jã', 1),\n",
       " ('comemoravam', 1),\n",
       " ('inesperado', 1),\n",
       " ('aconteceu', 1),\n",
       " ('marcadores', 1),\n",
       " ('seguravam', 1),\n",
       " ('camisa', 1),\n",
       " ('corrida', 1),\n",
       " ('wide', 1),\n",
       " ('receiver', 1),\n",
       " ('bem', 1),\n",
       " ('marcado', 1),\n",
       " ('saltou', 1),\n",
       " ('recepã', 1),\n",
       " ('quatro', 1),\n",
       " ('conseguir', 1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]\\w*')\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "nova_lista = []\n",
    "'''\n",
    "for token in tokens:\n",
    "  if token.lower() not in stopwords:\n",
    "   nova_lista.append(token.lower())'''\n",
    "nova_lista = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "#print(nova_lista)\n",
    "freq = nltk.FreqDist(nova_lista)\n",
    "freq.most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'jogador',\n",
       " 'que',\n",
       " 'esta',\n",
       " 'com',\n",
       " 'a',\n",
       " 'cmisa',\n",
       " 'marcou',\n",
       " 'o',\n",
       " 'gol',\n",
       " 'da',\n",
       " 'vitoria']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"o jogador, que esta com a cmisa 10, marcou o gol da vitoria!\"\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[A-z]\\w*')\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1b19edbd47396f742a121254609d057116f50353173c1f6d276cb8654ed8a8b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
